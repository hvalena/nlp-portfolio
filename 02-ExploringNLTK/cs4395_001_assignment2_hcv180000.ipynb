{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNsDPPR6qUXwZn7dZI2RUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hvalena/nlp-portfolio/blob/main/cs4395_001_assignment2_hcv180000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring NLTK\n",
        "Assignment 2  \n",
        "CS 4395.001: Human Language Technologies  \n",
        "Hannah Valena - HCV180000  "
      ],
      "metadata": {
        "id": "KGioYkZEmhHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "The imports and downloads below install Python's Natural Language Toolkit (NLTK), which we can use to perform natural language processing."
      ],
      "metadata": {
        "id": "52uec4adpVzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rvUnwxmHkq0b",
        "outputId": "7567eaa5-b7b3-44a6-ae92-24934deeb9d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1V3laRKjl5RP",
        "outputId": "3cf48cba-b475-47cb-aa19-ec9f8cde6893"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LPRKHW5GmMS7",
        "outputId": "152917ab-f2b2-4d88-ecd8-ad708b80d91b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokens()\n",
        "1.   The return type for the tokens() method for Text objects is a list.\n",
        "2.   Tokens() returns the tokens in a Text object.  \n",
        "  \n",
        "The code below extracts and prints the first 20 tokens from text1 (Moby Dick).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tIlXbnOGmeql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.tokens[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "un9PisZ0mQ-9",
        "outputId": "a3bebb60-442f-444e-b5ef-555bb5210231"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concordance()\n",
        "The code below uses the concordance() method of NLTK Text objects to find 5 occurrences of the word \"sea\""
      ],
      "metadata": {
        "id": "ro3mQlpxoinS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance(word=\"sea\", lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-_iqS7VJnc8R",
        "outputId": "19a5803e-33e8-4278-e256-1bdc74181965"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count()\n",
        "The **count()** method in the Text API counts the number of a times a word appears in a\n",
        "Text. It gets all of the tokens in the Text using tokens(), then it counts how many times\n",
        "the word, ie token, appears in the tokens list.  \n",
        "  \n",
        "This is different from Python’s **count** method. Python’s count() method can check for\n",
        "the number of occurrences of a substring of multiple words, while Text’s count()\n",
        "method checks for the occurrence of one token.  \n",
        "  \n",
        "This difference is shown in the code blocks below."
      ],
      "metadata": {
        "id": "Otk83P8lpalw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story_tokens = ['hi', 'this', 'is', 'my', 'story', '.', 'my', 'story', 'is', 'a', 'story', '.']\n",
        "story_text = Text(story_tokens)\n",
        "story_str = 'hi this is my story. my story is a story.'"
      ],
      "metadata": {
        "id": "BsfAs6pPAzDX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story_text.count('story')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rpvYosKyBoAo",
        "outputId": "5c737b1c-1e83-4000-ca64-e0047d02c2c6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_str.count('story')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ISDOO7frBqk0",
        "outputId": "5e5bc92b-905f-46a4-eba0-f5684cd93542"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_text.count('my story')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8cfOjrKaBtGp",
        "outputId": "c8c17fba-f8b3-4686-af9b-0b52e7bc2a40"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "story_str.count('my story')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vStR2bBnBvGi",
        "outputId": "9b8b5fcf-c890-4ceb-b2b5-cb8398929293"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word_tokenize()\n",
        "NLTK's word tokenizer separates text into individual tokens.  \n",
        "  \n",
        "The code below shows how word_tokenize() works using raw text input from the trasncript of Shrek, taken from the following website: https://shrek.fandom.com/wiki/Shrek_(film)/Transcript."
      ],
      "metadata": {
        "id": "fjiWkLLBCTD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "C0UNMKJsDmty"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = 'A masked man is pouring a glass of milk. Another man is shown walking down the hallway towards a set of doors. As he is let into the room by two guards, we can see that the man is abnormally short. The masked man is dunking what looks to be a small person into the glass of milk. The Gingerbread Man is pulled out of the milk by Thelonious and is slammed down onto a cookie sheet. Farquaad manically laughs as he walks over to the table. When he reaches the table we see that he is too short to see above it. He clears his throat and the table is lowered.'"
      ],
      "metadata": {
        "id": "JDfXk64cDKiO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(raw_text)\n",
        "\n",
        "# display the first 10 elements in tokens\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x2oWP-1ZDy5C",
        "outputId": "4a0833f1-df29-48c7-86a6-5a950050522e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'masked', 'man', 'is', 'pouring', 'a', 'glass', 'of', 'milk', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sent_tokenize()\n",
        "Similar to word_tokenize, sent_tokenize() separates text into sentences.  \n",
        "  \n",
        "The code below shows how sent_tokenize() works using the same raw_text input as above."
      ],
      "metadata": {
        "id": "N2qQrLEFEO3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize"
      ],
      "metadata": {
        "id": "_BDNe67KEWpr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(raw_text)\n",
        "for sent in sentences:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hLPFyMl_EkDS",
        "outputId": "90cac625-e42b-4845-eb31-4bc1f0dbd178"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A masked man is pouring a glass of milk.\n",
            "Another man is shown walking down the hallway towards a set of doors.\n",
            "As he is let into the room by two guards, we can see that the man is abnormally short.\n",
            "The masked man is dunking what looks to be a small person into the glass of milk.\n",
            "The Gingerbread Man is pulled out of the milk by Thelonious and is slammed down onto a cookie sheet.\n",
            "Farquaad manically laughs as he walks over to the table.\n",
            "When he reaches the table we see that he is too short to see above it.\n",
            "He clears his throat and the table is lowered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer()\n",
        "NTLK's PorterStemmer removes affixes from tokens to help normalize text.  \n",
        "  \n",
        "The below code demonstrates PorterStemmer() using the same raw_input text as above."
      ],
      "metadata": {
        "id": "Nr84aFpuEyg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stem each token using a list comprehension\n",
        "stemmed = [stemmer.stem(t) for t in tokens]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gTTza9lBEyQz",
        "outputId": "d7ffb115-a589-44cf-c9e5-aefb429ff31e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'mask', 'man', 'is', 'pour', 'a', 'glass', 'of', 'milk', '.', 'anoth', 'man', 'is', 'shown', 'walk', 'down', 'the', 'hallway', 'toward', 'a', 'set', 'of', 'door', '.', 'as', 'he', 'is', 'let', 'into', 'the', 'room', 'by', 'two', 'guard', ',', 'we', 'can', 'see', 'that', 'the', 'man', 'is', 'abnorm', 'short', '.', 'the', 'mask', 'man', 'is', 'dunk', 'what', 'look', 'to', 'be', 'a', 'small', 'person', 'into', 'the', 'glass', 'of', 'milk', '.', 'the', 'gingerbread', 'man', 'is', 'pull', 'out', 'of', 'the', 'milk', 'by', 'theloni', 'and', 'is', 'slam', 'down', 'onto', 'a', 'cooki', 'sheet', '.', 'farquaad', 'manic', 'laugh', 'as', 'he', 'walk', 'over', 'to', 'the', 'tabl', '.', 'when', 'he', 'reach', 'the', 'tabl', 'we', 'see', 'that', 'he', 'is', 'too', 'short', 'to', 'see', 'abov', 'it', '.', 'he', 'clear', 'hi', 'throat', 'and', 'the', 'tabl', 'is', 'lower', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordNetLemmatizer()\n",
        "Similar to PorterStemmer(), NTLK's WordNetLemmatizer() aims to normalize text. WordNetLemmatizer() tries to group different variations of a word to make analysis more straightforward. Lemmatization is less aggressive than stemming.  \n",
        "  \n",
        "The code below demonstrates WordNetLemmatizer() using the same raw_input text as above.  \n",
        "  \n",
        "Five differences between the stemmed vs. lemmatized (in the format stemmed-lemmatized) list are:  \n",
        "1. mask-masked\n",
        "2. pour-pouring\n",
        "3. anoth-Another\n",
        "4. walk-walking\n",
        "5. abnorm-abnormally"
      ],
      "metadata": {
        "id": "4X9bu-hcFyLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# stem each token using a list comprehension\n",
        "lemmatize = [wnl.lemmatize(t) for t in tokens]\n",
        "print(lemmatize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RcwAbbuaGBQI",
        "outputId": "910c392c-741b-4793-e350-6a4977e4df14"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'masked', 'man', 'is', 'pouring', 'a', 'glass', 'of', 'milk', '.', 'Another', 'man', 'is', 'shown', 'walking', 'down', 'the', 'hallway', 'towards', 'a', 'set', 'of', 'door', '.', 'As', 'he', 'is', 'let', 'into', 'the', 'room', 'by', 'two', 'guard', ',', 'we', 'can', 'see', 'that', 'the', 'man', 'is', 'abnormally', 'short', '.', 'The', 'masked', 'man', 'is', 'dunking', 'what', 'look', 'to', 'be', 'a', 'small', 'person', 'into', 'the', 'glass', 'of', 'milk', '.', 'The', 'Gingerbread', 'Man', 'is', 'pulled', 'out', 'of', 'the', 'milk', 'by', 'Thelonious', 'and', 'is', 'slammed', 'down', 'onto', 'a', 'cookie', 'sheet', '.', 'Farquaad', 'manically', 'laugh', 'a', 'he', 'walk', 'over', 'to', 'the', 'table', '.', 'When', 'he', 'reach', 'the', 'table', 'we', 'see', 'that', 'he', 'is', 'too', 'short', 'to', 'see', 'above', 'it', '.', 'He', 'clear', 'his', 'throat', 'and', 'the', 'table', 'is', 'lowered', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK Library\n",
        "Python's NLTK library seems to have many text processing functionalities, which makes Python a good choice of language for natural language processing. The code quality of Python's NLTK seems to be very high, which accounts for its popularity in both research and in education. In future projects, NLTK could be used to create a chatbot with predictive chat suggestions, a resume screening software, and even a sentiment analysis of various online forums/news."
      ],
      "metadata": {
        "id": "kGaMDfwaHT9Z"
      }
    }
  ]
}