{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet\n",
        "Assignment 4  \n",
        "CS 4395.001: Human Language Technologies  \n",
        "Hannah Valena - HCV180000"
      ],
      "metadata": {
        "id": "Q4XiEbCmoDwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordNet Summary\n",
        "WordNet is a database for the English language that is used to help with natural language processing. It hierarchically organizes nouns, verbs, adjectives, and adverbs, listing for each word:\n",
        "* glosses, which are short definitions of a word\n",
        "* synsets, which are synonym sets for a word\n",
        "* usage examples of a word\n",
        "* relations to other words "
      ],
      "metadata": {
        "id": "-fDXIy9HoSu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports to use nltk's wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AvDcBbCRqm0F",
        "outputId": "8bcc0983-5064-424f-9f6f-98f1b288e202"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organization of Nouns\n",
        "The top-level synset of nouns in WordNet is 'entity.n.01', as seen in the below output. As you traverse to higher levels, the higher level is either a hypernym (higher) or holonym (whole) of the level below. Conversely, the lower level is a hyponym (lower) or meronym (part of) of the level above.\n"
      ],
      "metadata": {
        "id": "VkptaWmkqTDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output all synsets of noun 'dragon'\n",
        "print(wn.synsets('dragon'))\n",
        "\n",
        "# selecting one synset of dragon\n",
        "dragon_synset = wn.synset('dragon.n.01')\n",
        "\n",
        "# extract synset definition, usage examples, and lemmas\n",
        "print('\\ndragon.n.01:')\n",
        "print(f'\\ndefinition: {dragon_synset.definition()}')\n",
        "print(f'examples: {dragon_synset.examples()}')\n",
        "print(f'lemmas: {dragon_synset.lemmas()}')\n",
        "\n",
        "# traverse the hierarchy\n",
        "print('\\nTraversing the hierarchy:')\n",
        "hyp = dragon_synset.hypernyms()[0]\n",
        "top = wn.synset('entity.n.01')\n",
        "while hyp:\n",
        "  print(hyp)\n",
        "  if hyp == top:\n",
        "    break;\n",
        "  if hyp.hypernyms():\n",
        "    hyp = hyp.hypernyms()[0]\n",
        "\n",
        "# hypernyms of dragon.n.01\n",
        "print(f'\\nHypernyms: {dragon_synset.hypernyms()}')\n",
        "print(f'Hyponyms: {dragon_synset.hyponyms()}')\n",
        "print(f'Meronyms {dragon_synset.part_meronyms()}')\n",
        "print(f'Holonyms: {dragon_synset.member_holonyms()}')\n",
        "print(f'Antonyms: {dragon_synset.lemmas()[0].antonyms()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NWA9rJS3rUDQ",
        "outputId": "6ca94e67-3dfa-4293-e86d-c1b1281eb6bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('dragon.n.01'), Synset('dragon.n.02'), Synset('draco.n.02'), Synset('dragon.n.04')]\n",
            "\n",
            "dragon.n.01:\n",
            "\n",
            "definition: a creature of Teutonic mythology; usually represented as breathing fire and having a reptilian body and sometimes wings\n",
            "examples: []\n",
            "lemmas: [Lemma('dragon.n.01.dragon'), Lemma('dragon.n.01.firedrake')]\n",
            "\n",
            "Traversing the hierarchy:\n",
            "Synset('mythical_monster.n.01')\n",
            "Synset('monster.n.01')\n",
            "Synset('imaginary_being.n.01')\n",
            "Synset('imagination.n.01')\n",
            "Synset('creativity.n.01')\n",
            "Synset('ability.n.02')\n",
            "Synset('cognition.n.01')\n",
            "Synset('psychological_feature.n.01')\n",
            "Synset('abstraction.n.06')\n",
            "Synset('entity.n.01')\n",
            "\n",
            "Hypernyms: [Synset('mythical_monster.n.01')]\n",
            "Hyponyms: [Synset('wyvern.n.01')]\n",
            "Meronyms []\n",
            "Holonyms: []\n",
            "Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organization of Verbs\n",
        "Unlike nouns, verbs do not have a common top-level synset in WordNet. As seen in the code below, the top level for  'cook' is 'make.v.03', while the top level for 'swim' is 'travel.v.01' "
      ],
      "metadata": {
        "id": "mEWryI10w0NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output all synsets of verb 'cook'\n",
        "print(wn.synsets('cook'))\n",
        "\n",
        "# selecting one synset of 'cook'\n",
        "cook_synset = wn.synset('cook.v.01')\n",
        "\n",
        "# extract synset definition, usage examples, and lemmas\n",
        "print('\\ncook.v.01:')\n",
        "print(f'\\ndefinition: {cook_synset.definition()}')\n",
        "print(f'examples: {cook_synset.examples()}')\n",
        "print(f'lemmas: {cook_synset.lemmas()}')\n",
        "\n",
        "# traverse the hierarchy\n",
        "print('\\nTraversing the hierarchy:')\n",
        "hyp = cook_synset.hypernyms()[0]\n",
        "top = wn.synset('make.v.03') # had to make top = make, otherwise infinite loop\n",
        "while hyp:\n",
        "  print(hyp)\n",
        "  if hyp == top:\n",
        "    break;\n",
        "  if hyp.hypernyms():\n",
        "    hyp = hyp.hypernyms()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F_49AFy8w6t8",
        "outputId": "62d70d6f-4347-40de-9e1c-d2525f29e8cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('cook.n.01'), Synset('cook.n.02'), Synset('cook.v.01'), Synset('cook.v.02'), Synset('cook.v.03'), Synset('fudge.v.01'), Synset('cook.v.05')]\n",
            "\n",
            "cook.v.01:\n",
            "\n",
            "definition: prepare a hot meal\n",
            "examples: [\"My husband doesn't cook\"]\n",
            "lemmas: [Lemma('cook.v.01.cook')]\n",
            "\n",
            "Traversing the hierarchy:\n",
            "Synset('create_from_raw_material.v.01')\n",
            "Synset('make.v.03')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the same with a different verb, 'swim':"
      ],
      "metadata": {
        "id": "71TgJmY2yWyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output all synsets of verb 'swim'\n",
        "print(wn.synsets('swim'))\n",
        "\n",
        "# selecting one synset of 'swim'\n",
        "swim_synset = wn.synset('swim.v.01')\n",
        "\n",
        "# extract synset definition, usage examples, and lemmas\n",
        "print('\\nswim.v.01:')\n",
        "print(f'\\ndefinition: {swim_synset.definition()}')\n",
        "print(f'examples: {swim_synset.examples()}')\n",
        "print(f'lemmas: {swim_synset.lemmas()}')\n",
        "\n",
        "# traverse the hierarchy\n",
        "print('\\nTraversing the hierarchy:')\n",
        "hyp = swim_synset.hypernyms()[0]\n",
        "top = wn.synset('travel.v.01') # had to make top = travel, otherwise infinite loop\n",
        "while hyp:\n",
        "  print(hyp)\n",
        "  if hyp == top:\n",
        "    break;\n",
        "  if hyp.hypernyms():\n",
        "    hyp = hyp.hypernyms()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xT3P38ZJx71h",
        "outputId": "c9e51cc5-a1a5-47df-83c0-08b4a003ec5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('swimming.n.01'), Synset('swim.v.01'), Synset('float.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swim.v.05')]\n",
            "\n",
            "swim.v.01:\n",
            "\n",
            "definition: travel through water\n",
            "examples: ['We had to swim for 20 minutes to reach the shore', 'a big fish was swimming in the tank']\n",
            "lemmas: [Lemma('swim.v.01.swim')]\n",
            "\n",
            "Traversing the hierarchy:\n",
            "Synset('travel.v.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Morphy\n",
        "Morphy finds and removes suffixes. For example, for nouns, morphy() removes 's', 'ses', etc. and for verbs, morphy() removes 's', 'ies', 'ed', 'ing', etc."
      ],
      "metadata": {
        "id": "_aXN82Gu4yeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Use Morphy to find different forms of \\'cook\\':')\n",
        "print('cook -> ' + wn.morphy('cook', wn.VERB))\n",
        "print('cooking -> ' + wn.morphy('cooking', wn.VERB))\n",
        "print('cooked -> ' + wn.morphy('cooked', wn.VERB))\n",
        "print('cooks -> ' + wn.morphy('cooks', wn.VERB))\n",
        "\n",
        "print('\\nUse Morphy to find different forms of \\'swim\\':')\n",
        "print('swim -> ' + wn.morphy('swim', wn.VERB))\n",
        "print('swimming -> ' +wn.morphy('swimming', wn.VERB))\n",
        "print('swam -> ' + wn.morphy('swam', wn.VERB))\n",
        "print('swum -> ' + wn.morphy('swum', wn.VERB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sKuDFKmt42lq",
        "outputId": "85a0951a-48a6-4592-981b-d5ba87b87a7b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use Morphy to find different forms of 'cook':\n",
            "cook -> cook\n",
            "cooking -> cook\n",
            "cooked -> cook\n",
            "cooks -> cook\n",
            "\n",
            "Use Morphy to find different forms of 'swim':\n",
            "swim -> swim\n",
            "swimming -> swim\n",
            "swam -> swim\n",
            "swum -> swim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wu-Palmer Similarity Metric &  Lesk Algorithm\n",
        "### Wu-Palmer\n",
        "The Wu-Palmer Similarity metric calculates the similarity from 0 (little) to 1 (identity) of two synsets.  \n",
        "  \n",
        "As you can see in the code below, the synsets for computer and laptop have a Wu-Palmer similarity of about 0.82, meaning they are quite similar.  \n",
        "  \n",
        "### Lesk Algorithm\n",
        "The Lesk Algorithm looks at the context of a word and compares it to dictionary glosses for word overlaps.  \n",
        "  \n",
        "As you can see in the code below, the word 'break' could be related to various synsets depending on the context."
      ],
      "metadata": {
        "id": "Kwep9Xfk7yml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "\n",
        "# similar words\n",
        "print(wn.synsets('computer'))\n",
        "print(wn.synsets('laptop'))\n",
        "\n",
        "comp_synset = wn.synset('computer.n.01')\n",
        "laptop_synset = wn.synset('laptop.n.01')\n",
        "\n",
        "# wu-palmer\n",
        "wp_sim = wn.wup_similarity(comp_synset, laptop_synset)\n",
        "print(f'\\nWu-Palmer Similarity Metric for computer vs. laptop: {wp_sim}')\n",
        "\n",
        "# lesk\n",
        "sent1 = \"He finally got his big break.\"\n",
        "sent2 = \"There was a break in the action.\"\n",
        "sent3 = \"There was a break in her voice.\"\n",
        "sent4 = \"She made a break for the door.\"\n",
        "print('\\nLesk Algorithm for \\'break\\':')\n",
        "print(f'\\n{sent1}')\n",
        "print(lesk(sent1.split(\" \"), 'break', 'n'))\n",
        "\n",
        "print(f'\\n{sent2}')\n",
        "print(lesk(sent2.split(\" \"), 'break', 'n'))\n",
        "\n",
        "print(f'\\n{sent3}')\n",
        "print(lesk(sent3.split(\" \"), 'break', 'n'))\n",
        "\n",
        "print(f'\\n{sent4}')\n",
        "print(lesk(sent4.split(\" \"), 'break', 'n'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YM5YFGnz8kFs",
        "outputId": "8d460739-59ff-498f-cf94-ec4f1b6ef7e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('computer.n.01'), Synset('calculator.n.01')]\n",
            "[Synset('laptop.n.01')]\n",
            "\n",
            "Wu-Palmer Similarity Metric for computer vs. laptop: 0.8181818181818182\n",
            "\n",
            "Lesk Algorithm for 'break':\n",
            "\n",
            "He finally got his big break.\n",
            "Synset('rupture.n.02')\n",
            "\n",
            "There was a break in the action.\n",
            "Synset('fault.n.04')\n",
            "\n",
            "There was a break in her voice.\n",
            "Synset('open_frame.n.01')\n",
            "\n",
            "She made a break for the door.\n",
            "Synset('fault.n.04')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentiWordNet\n",
        "SentiWordNet assigns 3 scores to a synset based on some analysis: positivity, negativity, and objectivity. It can be used for sentiment analysis of any text, and the scores it gives can be especially useful for analyzing customer feedback, marketing campaigns, product acceptance, etc.  \n",
        "  \n",
        "The code below demonstrates using SentiWordNet for the word 'passion', which has multiple different synsets of varying scores. The example sentence using the word 'passion' is overwhelmingly positive until the phrase \"despite her mother's discouragement\". SentiWordNet recognizes that 'despite' and 'discouragement' have high negative scores."
      ],
      "metadata": {
        "id": "RqtUYmJx_eTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "# select emotionally charged word\n",
        "print('Synsets for \\'passion\\':\\n')\n",
        "passion_senti = swn.senti_synsets('passion')\n",
        "passion_syn = wn.synsets('passion')\n",
        "for senti in passion_senti:\n",
        "  print(f'{senti}')\n",
        "  \n",
        "# output polarity for each word in a sentence\n",
        "sent = \"She cultivated her passion for dancing from a young age, despite her mother's discouragement.\"\n",
        "neg = 0\n",
        "pos = 0\n",
        "for word in sent.split():\n",
        "  syn_list = list(swn.senti_synsets(word))\n",
        "  if syn_list:\n",
        "    syn = syn_list[0]\n",
        "    neg += syn.neg_score()\n",
        "    pos += syn.pos_score()\n",
        "  print(f'word: {word}\\tneg: {neg}\\tpos: {pos} ')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o12dZwV__gU7",
        "outputId": "e58fd0a6-c98e-4496-de5c-cf782085649c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synsets for 'passion':\n",
            "\n",
            "<passion.n.01: PosScore=0.5 NegScore=0.0>\n",
            "<heat.n.04: PosScore=0.5 NegScore=0.125>\n",
            "<rage.n.03: PosScore=0.625 NegScore=0.0>\n",
            "<mania.n.01: PosScore=0.0 NegScore=0.125>\n",
            "<passion.n.05: PosScore=0.625 NegScore=0.0>\n",
            "<love.n.02: PosScore=0.375 NegScore=0.0>\n",
            "<passion.n.07: PosScore=0.0 NegScore=0.5>\n",
            "word: She\tneg: 0\tpos: 0 \n",
            "word: cultivated\tneg: 0.0\tpos: 0.375 \n",
            "word: her\tneg: 0.0\tpos: 0.375 \n",
            "word: passion\tneg: 0.0\tpos: 0.875 \n",
            "word: for\tneg: 0.0\tpos: 0.875 \n",
            "word: dancing\tneg: 0.0\tpos: 0.875 \n",
            "word: from\tneg: 0.0\tpos: 0.875 \n",
            "word: a\tneg: 0.0\tpos: 0.875 \n",
            "word: young\tneg: 0.125\tpos: 0.875 \n",
            "word: age,\tneg: 0.125\tpos: 0.875 \n",
            "word: despite\tneg: 0.75\tpos: 0.875 \n",
            "word: her\tneg: 0.75\tpos: 0.875 \n",
            "word: mother's\tneg: 0.75\tpos: 0.875 \n",
            "word: discouragement.\tneg: 0.75\tpos: 0.875 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations\n",
        "Collocations are when words are seen together more often than usually expected by chance. You cannot substitute a word with another word and retain the same meaning. For example, \"wild rice\" is not chaotic rice, and \"strong tea\" is not tea that frequents the gym.  \n",
        "  \n",
        "The code below shows the collocations found in text4, as well as calculations for the mutual information of the words \"God bless\".  \n",
        "  \n",
        "\"God bless\" has a point-wise mutual information (PMI) = 4.1, which indicates it is likely to be a collocation. A PMI of 0 means x and y are independent, and a negative PMI indicates the words are probably not a collocation."
      ],
      "metadata": {
        "id": "lJTGjPGgj42X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4PGfmrxZkeJ4",
        "outputId": "9597e505-d374-40b3-db41-a523a8799ea0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collocations for text 4: Inaugural Corpus\n",
        "print(text4.collocations())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ymw3YQh7k3tE",
        "outputId": "ae9b23e3-48dd-45a8-fa21-b420f2dc8eac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate mutual information for 'God bless'\n",
        "import math\n",
        "\n",
        "text4_tokens = ' '.join(text4.tokens)\n",
        "words = len(set(text4))\n",
        "\n",
        "god_bless = text4_tokens.count('God bless')/words\n",
        "print(f'p(God bless) = {god_bless}')\n",
        "\n",
        "god = text4_tokens.count('God')/words\n",
        "print(f'p(God) = {god}')\n",
        "\n",
        "bless = text4_tokens.count('bless')/words\n",
        "print(f'p(bless) = {bless}')\n",
        "\n",
        "pmi = math.log2(god_bless / (god * bless))\n",
        "print(f'pmi = {pmi}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d8NPqfjOlt2Z",
        "outputId": "c0a2333b-76a9-4d68-de0b-47208ff0267c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(God bless) = 0.0016957605985037406\n",
            "p(God) = 0.011172069825436408\n",
            "p(bless) = 0.0085785536159601\n",
            "pmi = 4.145157780720282\n"
          ]
        }
      ]
    }
  ]
}